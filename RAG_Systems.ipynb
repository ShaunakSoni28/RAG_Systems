{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaunakSoni28/RAG_Systems/blob/main/RAG_Systems.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Creating project directory structure\n",
        "import os\n",
        "project_dir = '/content/drive/MyDrive/RAG_Project/'\n",
        "os.makedirs(project_dir, exist_ok=True)\n",
        "os.makedirs(f'{project_dir}/papers', exist_ok=True)\n",
        "os.makedirs(f'{project_dir}/data', exist_ok=True)\n",
        "os.makedirs(f'{project_dir}/results', exist_ok=True)\n",
        "os.makedirs(f'{project_dir}/evaluation', exist_ok=True)\n",
        "\n",
        "print(f\"‚úÖ Project directory: {project_dir}\")\n",
        "print(\"‚úÖ All work will be saved to Google Drive!\")\n",
        "print(\"‚úÖ Safe from disconnects!\")"
      ],
      "metadata": {
        "id": "3qBp4ViAghFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIXjDohCR7jc"
      },
      "outputs": [],
      "source": [
        "# Installing required libraries\n",
        "!pip install -q transformers accelerate sentence-transformers faiss-cpu pypdf langchain huggingface_hub\n",
        "\n",
        "# Downloading papers directly in Colab\n",
        "!pip install arxiv\n",
        "\n",
        "\n",
        "# Importing basic libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import arxiv\n",
        "\n",
        "print(\"‚úÖ Setup complete!\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "project_dir = '/content/drive/MyDrive/RAG_Project/'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wic1eb4zSNpt"
      },
      "outputs": [],
      "source": [
        "\n",
        "import arxiv\n",
        "import os\n",
        "\n",
        "# Creating the directory if it doesn't exist\n",
        "os.makedirs(\"/content/drive/MyDrive/RAG_Project/papers\", exist_ok=True)\n",
        "\n",
        "# Searching for NLP papers\n",
        "search = arxiv.Search(\n",
        "    query=\"cat:cs.CL\",  # Computer Science - Computation and Language\n",
        "    max_results=50,\n",
        "    sort_by=arxiv.SortCriterion.SubmittedDate\n",
        ")\n",
        "\n",
        "papers = []\n",
        "for result in search.results():\n",
        "    papers.append({\n",
        "        'title': result.title,\n",
        "        'pdf_url': result.pdf_url,\n",
        "        'summary': result.summary,\n",
        "        'authors': [author.name for author in result.authors]\n",
        "    })\n",
        "    # Downloading PDF\n",
        "    result.download_pdf(filename=f\"/content/drive/MyDrive/RAG_Project/papers/{result.get_short_id()}.pdf\")\n",
        "\n",
        "print(f\"‚úÖ Downloaded {len(papers)} papers!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pyPDF2\n",
        "\n",
        "from PyPDF2 import PdfReader\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "  try:\n",
        "    reader = PdfReader(pdf_path)\n",
        "    text=\"\"\n",
        "    for page in reader.pages:\n",
        "      text += page.extract_text() + \"\\n\"\n",
        "    return text.strip()\n",
        "  except Exception as e:\n",
        "    print(f\"Error with {pdf_path}: {e}\")\n",
        "    return \"\"\n",
        "\n",
        "print(\"\\n Processing 50 Downloaded Papers!\")\n",
        "all_papers=[]\n",
        "\n",
        "paper_files = [f for f in os.listdir(f\"{project_dir}papers/\") if f.endswith(\".pdf\") and not f.startswith(\"distractor_\")]\n",
        "\n",
        "for pdf_file in tqdm(paper_files, desc=\"Processing PDFs\"):\n",
        "  pdf_path = f\"{project_dir}/papers/{pdf_file}\"\n",
        "  text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "  if text and len(text.split()) > 100:\n",
        "    all_papers.append({\n",
        "        'filename ' : pdf_file,\n",
        "        'text' : text,\n",
        "        'word_count' : len(text.split()),\n",
        "        'is_distractor' : False\n",
        "    })\n",
        "\n",
        "    print(f\"Succesfuly processed {len(all_papers)} papers!\")\n",
        "    print(f\"Avergae words per paper: {sum(p['word_count'] for p in all_papers)//len(all_papers)}\")\n",
        "\n",
        "    # Saving the files in the drive\n",
        "\n",
        "    with open(f'{project_dir}data/main_papers.pkl','wb') as f:\n",
        "      pickle.dump(all_papers,f)\n",
        "    print(f\"Saved in Google Drive: {project_dir}data/main_papers.pkl\")"
      ],
      "metadata": {
        "id": "_Nua4MWijofD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Downloading 100 distractor papers from broader AI topics...\")\n",
        "\n",
        "# Distractor Papers\n",
        "\n",
        "distractor_queries=[\n",
        "    \"cat:cs.AI\", # Artificial Intelligence\n",
        "    \"cat:cs.LG\", # Machine Learning\n",
        "    \"cat:cs.CV\", # Computer Vision\n",
        "]\n",
        "\n",
        "distractor_count = 0\n",
        "\n",
        "target_distractor = 100\n",
        "downloads_ids = set() # creating a set that will help to store the ids of the distractor sequenctially\n",
        "\n",
        "for query in distractor_queries:\n",
        "  if distractor_count >= target_distractor:\n",
        "    break\n",
        "\n",
        "  print(\"Searching...\")\n",
        "\n",
        "  search = arxiv.Search(\n",
        "      query = query ,\n",
        "      max_results = 40 ,\n",
        "      sort_by = arxiv.SortCriterion.SubmittedDate, # Here we are sorthing the data according to the publishing/submitting date\n",
        "  )\n",
        "\n",
        "  for result in search.results():\n",
        "    if distractor_count >= target_distractor:\n",
        "      break\n",
        "\n",
        "    paper_id = result.get_short_id() # getting the paper id\n",
        "\n",
        "    if paper_id == downloads_ids: # If the paper is downloaded then skip it\n",
        "      continue\n",
        "\n",
        "    try:\n",
        "      filename = f\"/content/drive/MyDrive/RAG_Project/papers/distractor_{paper_id}.pdf\"\n",
        "\n",
        "      if os.path.exists(filename):\n",
        "        distractor_count += 1\n",
        "        downloads_ids.add(paper_id)\n",
        "        continue\n",
        "\n",
        "      result.download_pdf(filename = filename)\n",
        "      downloads_ids.add(paper_id)\n",
        "      distractor_count +=1\n",
        "\n",
        "      if distractor_count % 10 == 0 :\n",
        "        print(f\"Downloaded {distractor_count}/{target_distractor}\")\n",
        "\n",
        "    except Exception as e:\n",
        "      print(f\"Failed to download {paper_id}: {e}\")\n",
        "      continue\n",
        "\n",
        "    print(f\"Total paper downloaded {distractor_count}\")"
      ],
      "metadata": {
        "id": "H79UNytxo_uH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jd4mWKEGSdu1"
      },
      "outputs": [],
      "source": [
        "# 1. SETUP: Load embedding model\n",
        "print(\"Loading embedding model...\")\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "print(\"‚úÖ Embedding model loaded!\")\n",
        "\n",
        "# 2. PREPARE: Chunk your documents\n",
        "def chunk_text(text, chunk_size=500, overlap=50):\n",
        "    \"\"\"Split text into overlapping chunks\"\"\"\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "\n",
        "    for i in range(0, len(words), chunk_size - overlap):\n",
        "        chunk = ' '.join(words[i:i + chunk_size])\n",
        "        if chunk.strip():  # Only add non-empty chunks\n",
        "            chunks.append(chunk)\n",
        "\n",
        "    # If no chunks created (text too short), use the whole text\n",
        "    if len(chunks) == 0:\n",
        "        chunks = [text.strip()]\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Example with one paper - LONGER sample text\n",
        "sample_text = \"\"\"\n",
        "Attention mechanisms have become integral to sequence modeling tasks in natural language processing.\n",
        "The Transformer architecture, introduced in the paper Attention is All You Need, relies entirely on self-attention mechanisms\n",
        "to compute representations of input and output sequences without using recurrent or convolutional layers.\n",
        "BERT uses bidirectional transformers for language understanding and has achieved state-of-the-art results on many NLP benchmarks.\n",
        "The key innovation of transformers is the multi-head attention mechanism which allows the model to jointly attend to information\n",
        "from different representation subspaces at different positions. This enables the model to capture long-range dependencies more\n",
        "effectively than traditional RNNs or LSTMs. GPT models use a decoder-only transformer architecture and are trained using\n",
        "a language modeling objective. These models have shown impressive performance on various downstream tasks through fine-tuning\n",
        "or few-shot learning. Recent work has focused on making transformers more efficient through techniques like sparse attention,\n",
        "linear attention mechanisms, and improved positional encodings. The scalability of transformers has enabled training of very\n",
        "large language models with billions of parameters that demonstrate emergent capabilities on complex reasoning tasks.\n",
        "\"\"\"\n",
        "\n",
        "# Use smaller chunk size for this demo\n",
        "chunks = chunk_text(sample_text, chunk_size=50, overlap=10)\n",
        "print(f\"‚úÖ Created {len(chunks)} chunks\")\n",
        "print(f\"Sample chunk 1: {chunks[0][:100]}...\")\n",
        "if len(chunks) > 1:\n",
        "    print(f\"Sample chunk 2: {chunks[1][:100]}...\")\n",
        "\n",
        "# 3. INDEX: Create FAISS vector database\n",
        "print(\"\\nCreating embeddings...\")\n",
        "chunk_embeddings = embedding_model.encode(chunks)\n",
        "print(f\"‚úÖ Embeddings shape: {chunk_embeddings.shape}\")\n",
        "\n",
        "# Build FAISS index\n",
        "dimension = chunk_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(chunk_embeddings)\n",
        "print(f\"‚úÖ FAISS index created with {index.ntotal} vectors\")\n",
        "\n",
        "# 4. RETRIEVE: Search function\n",
        "def retrieve_relevant_chunks(query, top_k=3):\n",
        "    \"\"\"Retrieve most relevant chunks for a query\"\"\"\n",
        "    query_embedding = embedding_model.encode([query])\n",
        "\n",
        "    # Make sure we don't ask for more chunks than we have\n",
        "    top_k = min(top_k, len(chunks))\n",
        "\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "\n",
        "    results = []\n",
        "    for idx, dist in zip(indices[0], distances[0]):\n",
        "        results.append({\n",
        "            'chunk': chunks[idx],\n",
        "            'distance': float(dist),\n",
        "            'chunk_id': int(idx)\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "# Test retrieval\n",
        "test_query = \"What are attention mechanisms?\"\n",
        "results = retrieve_relevant_chunks(test_query, top_k=3)\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"‚úÖ RETRIEVAL TEST\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Query: {test_query}\\n\")\n",
        "for i, result in enumerate(results, 1):\n",
        "    print(f\"{i}. Distance: {result['distance']:.4f}\")\n",
        "    print(f\"   Chunk: {result['chunk'][:150]}...\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. GENERATE: Use Flan-T5 (works immediately, no authentication needed)\n",
        "from transformers import pipeline\n",
        "\n",
        "print(\"Loading Flan-T5 model...\")\n",
        "generator = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=\"google/flan-t5-base\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "print(\"‚úÖ Flan-T5 loaded!\")\n",
        "\n",
        "def generate_answer(query, retrieved_chunks):\n",
        "    \"\"\"Generate answer using retrieved context\"\"\"\n",
        "\n",
        "    # Combine retrieved chunks into context\n",
        "    context = \"\\n\\n\".join([chunk['chunk'] for chunk in retrieved_chunks])\n",
        "\n",
        "    # Create prompt - Flan-T5 uses simpler format\n",
        "    prompt = f\"\"\"Answer the question based on the context below.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    # Generate\n",
        "    response = generator(\n",
        "        prompt,\n",
        "        max_length=256,\n",
        "        temperature=0.7,\n",
        "        do_sample=True\n",
        "    )\n",
        "\n",
        "    return response[0]['generated_text']\n",
        "\n",
        "# Test the full pipeline\n",
        "test_query = \"What are attention mechanisms?\"\n",
        "print(f\"\\nüîç Query: {test_query}\")\n",
        "\n",
        "# Retrieve\n",
        "retrieved = retrieve_relevant_chunks(test_query, top_k=3)\n",
        "print(f\"\\nüìö Retrieved {len(retrieved)} chunks\")\n",
        "\n",
        "# Generate\n",
        "answer = generate_answer(test_query, retrieved)\n",
        "print(f\"\\nüí° Answer: {answer}\")"
      ],
      "metadata": {
        "id": "VksrwsqjYscc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPLETE DEMO FUNCTION\n",
        "def rag_qa_system(question):\n",
        "    \"\"\"Complete RAG QA pipeline\"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(f\"QUESTION: {question}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Step 1: Retrieve\n",
        "    print(\"\\nüîç RETRIEVING relevant documents...\")\n",
        "    retrieved_chunks = retrieve_relevant_chunks(question, top_k=3)\n",
        "\n",
        "    for i, chunk in enumerate(retrieved_chunks, 1):\n",
        "        print(f\"\\n  [{i}] Similarity: {1/(1+chunk['distance']):.3f}\")\n",
        "        print(f\"      {chunk['chunk'][:100]}...\")\n",
        "\n",
        "    # Step 2: Generate\n",
        "    print(\"\\n\\nüí≠ GENERATING answer...\")\n",
        "    answer = generate_answer(question, retrieved_chunks)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ANSWER:\")\n",
        "    print(\"=\"*60)\n",
        "    print(answer)\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    return answer\n",
        "\n",
        "# Demo questions\n",
        "demo_questions = [\n",
        "    \"What are attention mechanisms in transformers?\",\n",
        "    \"How does BERT work?\",\n",
        "    \"What is the difference between GPT and BERT?\"\n",
        "]\n",
        "\n",
        "for q in demo_questions:\n",
        "    rag_qa_system(q)\n",
        "    print(\"\\n\\n\")"
      ],
      "metadata": {
        "id": "5EfYQt8hZK1w"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNI7ebQYNWqJWULQ9u6y5lf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}